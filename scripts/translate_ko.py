import os
import re
import yaml
import hashlib
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from deep_translator import GoogleTranslator

# ---------------------------------------------
# CONFIG
# ---------------------------------------------
WORKSPACE = os.getcwd()
SRC_DIR = os.path.join(WORKSPACE, "_posts")
DEST_DIR = os.path.join(WORKSPACE, "ko", "_posts")
os.makedirs(DEST_DIR, exist_ok=True)

CACHE_FILE = os.path.join(WORKSPACE, "translation_cache_ko.json")
MAX_WORKERS = 8
RETRY_COUNT = 2
RETRY_DELAY = 0.5  # seconds between retries

translator = GoogleTranslator(source='ja', target='ko')

# ---------------------------------------------
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ï¼ˆå­˜åœ¨ã—ãªãã¦ã‚‚å•é¡Œãªã„ï¼‰
# ---------------------------------------------
if os.path.exists(CACHE_FILE):
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            TRANSLATION_CACHE = json.load(f)
    except Exception:
        TRANSLATION_CACHE = {}
else:
    TRANSLATION_CACHE = {}

# ---------------------------------------------
# é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç¿»è¨³ã‹ã‚‰å®Œå…¨ã«ä¿è­·ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
# - NOTE: mermaid ã¯ã“ã“ã«å…¥ã‚Œãªã„ï¼ˆä¸­ã‚’è§£æã—ã¦ç¿»è¨³ã™ã‚‹ï¼‰
# ---------------------------------------------
EXCLUDE_BLOCK_PATTERNS = [
    # style / script / table / iframe / fenced code (``` ``` / ~~~ ~~~)
    (r"<style[\s\S]*?</style>", "STYLE"),
    (r"<script[\s\S]*?</script>", "SCRIPT"),
    (r"<table[\s\S]*?</table>", "TABLE"),
    (r"<iframe[\s\S]*?</iframe>", "IFRAME"),
    # fenced code blocks: ```lang ... ```  ã¾ãŸã¯ ~~~lang ... ~~~
    (r"```[\w\-]*[\s\S]*?```", "CODE_FENCE"),
    (r"~~~[\w\-]*[\s\S]*?~~~", "CODE_FENCE2"),
]

# Compile flags
COMPILED_PATTERNS = [(re.compile(pat, re.IGNORECASE | re.DOTALL), tag) for pat, tag in EXCLUDE_BLOCK_PATTERNS]

# ---------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£: ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡ºï¼ˆé‡ãªã‚Šå¯¾å‡¦ï¼‰
# - å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒƒãƒä½ç½®ã‚’åé›†ã—ã¦ã€å¾Œã‚ã‹ã‚‰ç½®æ›ã™ã‚‹æ–¹å¼ã§å®‰å…¨ã«ç½®æ›
# ---------------------------------------------
def extract_excluded_blocks(text):
    matches = []
    for pat, tag in COMPILED_PATTERNS:
        for m in pat.finditer(text):
            matches.append((m.start(), m.end(), m.group(0), tag))
    # ã‚½ãƒ¼ãƒˆã—ã¦å¾Œã‚ã‹ã‚‰ç½®æ›ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãšã‚Œé˜²æ­¢ï¼‰
    matches.sort(key=lambda x: x[0])
    placeholders = {}
    new_text = text
    offset = 0
    for i, (s, e, block, tag) in enumerate(matches):
        ph = f"__EXCL_{tag}_{i}__"
        # adjust for previous replacements
        s_adj = s + offset
        e_adj = e + offset
        new_text = new_text[:s_adj] + ph + new_text[e_adj:]
        placeholders[ph] = block
        offset += len(ph) - (e - s)
    return new_text, placeholders

def restore_excluded_blocks(text, placeholders):
    # å˜ç´”ç½®æ›ã§æˆ»ã™ï¼ˆplaceholder ã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰
    for ph, block in placeholders.items():
        text = text.replace(ph, block)
    return text

# ---------------------------------------------
# ç¿»è¨³ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ããƒ»ãƒªãƒˆãƒ©ã‚¤ãƒ»Noneå®‰å…¨ï¼‰
# ---------------------------------------------
def cached_translate(text: str) -> str:
    # ã‚­ãƒ¼ã¯ text ãã®ã‚‚ã®ã® MD5
    key = hashlib.md5(text.encode("utf-8")).hexdigest()
    if key in TRANSLATION_CACHE:
        return TRANSLATION_CACHE[key]
    # é˜²å¾¡: ç©ºæ–‡å­—ã¯ãã®ã¾ã¾ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦è¿”ã™
    if text.strip() == "":
        TRANSLATION_CACHE[key] = text
        return text
    # ãƒªãƒˆãƒ©ã‚¤
    last_exc = None
    for attempt in range(1, RETRY_COUNT + 2):  # 1 + RETRY_COUNT attempts
        try:
            translated = translator.translate(text)
            if not translated:
                translated = text
            TRANSLATION_CACHE[key] = translated
            return translated
        except Exception as e:
            last_exc = e
            time.sleep(RETRY_DELAY)
    # å¤±æ•—æ™‚ã¯å…ƒæ–‡ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦è¿”ã™
    TRANSLATION_CACHE[key] = text
    print(f"[WARN] translation failed, cached original. snippet: {text[:60]!r}, error: {last_exc}")
    return text

# ---------------------------------------------
# Mermaid è¡Œã®ç¿»è¨³ (ã‚³ãƒ¡ãƒ³ãƒˆã¨ãƒ©ãƒ™ãƒ«ã‚’ç„¡æ¡ä»¶ç¿»è¨³)
# ---------------------------------------------
def translate_mermaid_line(line: str) -> str:
    # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œ '%% comment'
    def repl_comment(m):
        return "%% " + cached_translate(m.group(1))
    line = re.sub(r"%%\s*(.*)", repl_comment, line)

    # ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«ãƒ»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç­‰ã‚’ç„¡æ¡ä»¶ç¿»è¨³
    patterns = [
        r'(\[)(.*?)(\])',      # [label]
        r'(\()([^()]*)(\))',   # (label)
        r'(\(\()([^()]*)(\)\))', # ((label))
        r'(\|)(.*?)(\|)',      # |label|
    ]
    for pat in patterns:
        line = re.sub(pat, lambda m: f"{m.group(1)}{cached_translate(m.group(2))}{m.group(3)}", line)
    return line

# ---------------------------------------------
# front matter åˆ†é›¢
# ---------------------------------------------
def split_front_matter(content: str):
    if content.startswith("---"):
        parts = content.split('---', 2)
        if len(parts) >= 3:
            return parts[1], parts[2]
    return "", content

def load_yaml_safe(fm: str):
    try:
        return yaml.safe_load(fm) or {}
    except Exception:
        return {}

def extract_slug(filename: str) -> str:
    base = os.path.splitext(filename)[0]
    base = re.sub(r'^\d{4}-\d{2}-\d{2}-', '', base)
    slug = re.sub(r'[^\w]+', '-', base)
    return slug.lower().strip('-')

# ---------------------------------------------
# å€‹åˆ¥è¨˜äº‹ç¿»è¨³ (ä¸»è¦å‡¦ç†)
# ---------------------------------------------
def translate_article(filename: str) -> str:
    src_path = os.path.join(SRC_DIR, filename)
    dest_path = os.path.join(DEST_DIR, filename)
    try:
        with open(src_path, "r", encoding="utf-8") as f:
            content = f.read()
    except Exception as e:
        print(f"[ERROR] cannot read {src_path}: {e}")
        return filename

    # 1) é™¤å¤–ãƒ–ãƒ­ãƒƒã‚¯ã‚’é€€é¿ï¼ˆiframe / code fences / style / script / tableï¼‰
    tmp, placeholders = extract_excluded_blocks(content)

    # 2) front matter ã¨ body ã‚’åˆ†é›¢
    fm, body = split_front_matter(tmp)
    fm_dict = load_yaml_safe(fm)

    # 3) front matter ã® title ã¯ç¿»è¨³ã€‚ãŸã ã—ä»–ã¯ãã®ã¾ã¾ä¿æŒ
    if fm_dict.get("title"):
        try:
            fm_dict["title"] = cached_translate(str(fm_dict["title"]))
        except Exception:
            pass

    slug = extract_slug(filename)
    fm_dict["lang"] = "ko"
    fm_dict["permalink"] = f"/ko/{slug}/"

    # 4) æœ¬æ–‡ç¿»è¨³ï¼šMermaid ãƒ–ãƒ­ãƒƒã‚¯ã¯ãƒãƒ¼ãƒ‰å˜ä½ã§ç¿»è¨³ï¼ˆdiv mermaid ã®ä¸­èº«ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
    translated_lines = []
    in_code = False
    in_mermaid = False

    for line in body.splitlines():
        # ãƒ•ã‚§ãƒ³ã‚¹è¡Œã®åˆ¤å®šï¼ˆãƒãƒƒã‚¯ãƒ†ã‚£ãƒƒã‚¯ç³»ï¼‰
        if line.strip().startswith("```") or line.strip().startswith("~~~"):
            in_code = not in_code
            translated_lines.append(line)
            continue
        if in_code:
            # code fence ä¸­ã¯ç¿»è¨³ã—ãªã„ï¼ˆæŠœãå‡ºã—æ¸ˆã¿ã® fenced code ã‚‚ã‚ã‚‹ãŒäºŒé‡ä¿è­·ï¼‰
            translated_lines.append(line)
            continue

        # Mermaid start detection: "graph" or "flowchart" è¡Œã€ã¾ãŸã¯ <div class="mermaid"> ã‚’å«ã‚€å ´åˆ
        trimmed = line.strip().lower()
        if trimmed.startswith("graph") or trimmed.startswith("flowchart") or "class=\"mermaid\"" in line.lower() or "class='mermaid'" in line.lower():
            in_mermaid = True
            translated_lines.append(line)
            continue

        if in_mermaid:
            # mermaid ãƒ–ãƒ­ãƒƒã‚¯çµ‚ç«¯ã¯ç©ºè¡Œã¾ãŸã¯ div close ã‚’æ¤œå‡ºï¼ˆHTMLã¨æ··åœ¨ã®å ´åˆã‚’åŒ…æ‹¬ï¼‰
            if trimmed == "" or trimmed == "</div>":
                if trimmed == "</div>":
                    in_mermaid = False
                translated_lines.append(line)
                continue
            # mermaid å†…è¡Œã¯ãƒãƒ¼ãƒ‰å˜ä½ã§ç¿»è¨³
            translated_lines.append(translate_mermaid_line(line))
            continue

        # é€šå¸¸è¡Œã¯ç„¡æ¡ä»¶ã§ç¿»è¨³ï¼ˆç©ºè¡Œã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã•ã‚Œã‚‹ãŒå•é¡Œãªã—ï¼‰
        translated_lines.append(cached_translate(line))

    translated_body = "\n".join(translated_lines)

    # 5) front matter ã‚’ YAML ã«æˆ»ã—ã¦ã€é€€é¿ã—ã¦ã„ãŸé™¤å¤–ãƒ–ãƒ­ãƒƒã‚¯ã‚’å¾©å…ƒ
    final = f"---\n{yaml.safe_dump(fm_dict, allow_unicode=True)}---\n{translated_body}"
    final = restore_excluded_blocks(final, placeholders)

    # 6) ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ï¼ˆå­˜åœ¨ã—ãªã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ä½œã‚‹ï¼‰
    try:
        os.makedirs(os.path.dirname(dest_path), exist_ok=True)
        with open(dest_path, "w", encoding="utf-8") as f:
            f.write(final)
        print(f"âœ… Translated: {filename} -> {os.path.abspath(dest_path)} ({len(final)} bytes)")
    except Exception as e:
        print(f"[ERROR] failed to write {dest_path}: {e}")

    return filename

# ---------------------------------------------
# ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒª
# ---------------------------------------------
def main():
    # sanity checks
    if not os.path.exists(SRC_DIR):
        print(f"[ERROR] SRC_DIR not found: {SRC_DIR}")
        return

    md_files = [f for f in os.listdir(SRC_DIR) if f.endswith(".md")]
    print(f"ğŸ”„ Translating {len(md_files)} articles with {MAX_WORKERS} workers...")
    if not md_files:
        print("[WARN] No markdown files found in _posts. Exiting.")
        return

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(translate_article, f): f for f in md_files}
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"[ERROR] processing {futures[future]}: {e}")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆä¸¦åˆ—å¾Œï¼‰
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(TRANSLATION_CACHE, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ‰ All translations done. Cache size: {len(TRANSLATION_CACHE)} entries -> {CACHE_FILE}")
    except Exception as e:
        print(f"[ERROR] failed to save cache: {e}")
