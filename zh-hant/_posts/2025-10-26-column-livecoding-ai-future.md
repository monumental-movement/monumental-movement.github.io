---
author: mmr
categories:
- Column
image: ../assets/images/column-livecoding-ai-future.webp
lang: zh-hant
layout: post
permalink: /zh-hant/column-livecoding-ai-future/
tags:
- Ai
- Experimental
- Live Coding
title: 【專欄】Live Coding與AI音樂的親和力與未來
---


## 序言：Code 登台之日


文字：mmr |主題：演奏由人工智能生成的和弦和音樂的文化。探索那個十字路口發生的創造性變化。

在俱樂部的黑暗中，屏幕上出現的不是音符而是和弦。
`d1 $ sound "bd sn [hh*2]"`──這不是樂譜，而是**即興算法**。

這種被稱為“實時編碼”的文化於 2000 年代初誕生於英國謝菲爾德。
藝術家在舞台上實時編寫程序並立即將其輸出為聲音。
音樂與編碼、俱樂部文化與算法的融合。
這種新的表達方式後來與人工智能音樂產生了深刻的共鳴。

---

<style type="text/css">

table, td, th {
border: 2px #111 solid;
width: auto;
padding: 10px; 
}
th {
background-color: #111;
color: #fff;
}
</style>


## 第一章：“演奏”和弦文化的誕生

實時編碼的起源在於**算法組合**。
最早的例子包括 Lejaren Hiller 和 Iannis Xenakis 在 20 世紀 50 年代的自動作曲實驗。
實時編碼將這一點帶入了 21 世紀，並帶回了物理性和實時功能。

2004年，社區“TOPLAP”由Alex McLean和Nick Collins提出。
口號是“向我們展示你的屏幕！”
通過與觀眾分享產生聲音的過程（代碼），
這個想法是將製作過程本身變成一場表演。

TidalCycles、SuperCollider 和 Sonic Pi 等環境
實現即興“手寫聲音”的行為，
它給電子音樂帶來了新的現場性質。

---

## 第二章：人工智能帶來的世代變革

在人工智能音樂的背景下，**使用深度學習的音樂生成**在 2010 年代末取得了重大進展。
代表性的例子包括 OpenAI 的“Jukebox”、Google 的“Magenta”和“Riffusion”。

AI不寫代碼。
相反，它從大量數據中學習模式並“內化”生產規則。
換句話說，人工智能是實時編碼“之外”的算法智能。
然而，近年來，界限迅速變得模糊。

例如，TidalCycles用戶使用GPT實時建議代碼，
人工智能分析現場表演並預測下一個節奏的案例已經開始出現。
這種融合預示著人工智能將成為實時編碼的聯合明星的未來。

---

## 第三章：人類即興創作與機器“即興創作”之間的差異

人類現場編碼員**享受錯誤和機會**。
意想不到的聲音和誤解驅動著音樂。
相比之下，AI即興創作是**根據過去的數據進行“重構”**，
本質上它仍然在概率範圍內。

然而，這種差異也是創造力的源泉。
人工智能提供了無限的組合，人類在其中找到了意義。
兩者之間不是一種“主從”關係，而是一種相互補充的創造性關係。

---

## 第4章：主要工具的演變和比較

|工具名稱 |開發商/組織|特點|人工智能協作的可能性 |
|------------|----------------|------|----------------|
| **潮汐週期** |亞歷克斯·麥克萊恩 |基於 Haskell 的實時編碼環境，專門用於模式描述 |通過 ChatGPT 集成可實現實時代碼生成 |
| **超級對撞機** |詹姆斯·麥卡特尼 |聲音合成和算法合成的歷史悠久的環境 |利用AI模型進行聲音參數控制 |
| **索尼克 Pi** |薩姆·亞倫 |基於 Ruby，兼顧教育和性能 |教育環境中使用的人工智能輔助代碼示例 |
| **擴散** |塞斯·福斯格倫等人。 |生成頻譜圖的擴散模型 | AI本身直接產生聲音|
| **豹貓/九頭蛇** |集成生成視頻+聲音的實時編碼環境|視覺和聲音的人工智能同步成為可能 |

---

## 第 5 章：人工智能與實時編碼之間的協作示例

- **AI-DJ 實驗（2023 年，柏林 CTM 音樂節）**
人類 Live Coder 在 TidalCycles 上演奏，人工智能分析 BPM、和聲和空間安排。
實時生成響應式混音。
結果，我們能夠以AI遵循“人類節奏”的形式共同主演。

- **Algorave × GPT Jam（2024 年，東京）**
多名現場編碼員在舞台上收到基於 GPT 的代碼建議，
一邊執行一邊現場修正。來自觀眾的聊天被用作輸入數據。
有人試圖讓人工智能解讀“這個地方的氣氛”。

- **擴散＋潮汐循環**
TidalCycles隨機重新排列AI生成的碎片聲音，
**人工智能主管“材料”、人類主管“結構”的新生產業態**。

---

## 第六章：道德與創造力——自動化時代的“表演者”意味著什麼？

AI寫代碼的版權歸誰所有？
“原創”的概念在即興創作中是否成立？

這些問題與實時編碼的哲學密切相關。
TOPLAP“開放流程”的理念是
**透明度=創造力的民主化**。
當人工智能加入這種文化時，
我們要抵制“黑匣子”。

如果生成式人工智能提出了代碼，那麼它的學習過程和決策標準也應該公開。
這是開啟算法音樂未來的關鍵。

---

## 第七章：未來展望——邁向“算法協作”

2030 年代，“AI 會話”將在音樂製作中變得司空見慣。
AI不僅僅是一個工具，而是一個協同執行者。
人類引導觀念和情感的方向，
人工智能即興創作數百種聲音模式。
從那裡進行選擇和編輯的行為本身就是一種“表演”。

此外，通過將實時編碼環境與人工智能集成，
還有一種可能，它會成為“提示=性能界面”。
不再需要鼠標或 MIDI。
**語言和思想本身變成聲音的時代**即將到來。

---

## 插圖：Live Coding × AI 進化時間表

<div class="mermaid">

timeline
    title Live CodingとAIミュージックの進化（2000–2025）
    2000 : TOPLAP結成、Live Coding文化が誕生
    2004 : TidalCycles初期版登場
    2016 : Deep Learning音楽生成モデル（Magenta, OpenAI MuseNet）
    2020 : RiffusionがAIスペクトログラム生成を開始
    2023 : AI × Live Coding共演イベントが欧州で拡大
    2025 : GPTベースのリアルタイムLive Coding環境登場



</div>

---

## 關聯圖：Live Coder 與 AI 的協作結構


<div class="mermaid">

flowchart TD
    A["人間（Live Coder）"] -->|コード入力・即興| B["Live Coding環境（Tidal, SuperCollider）"]
    B -->|生成音の出力| C["AI解析モジュール（テンポ・構造分析）"]
    C -->|予測・提案| D["AI生成器（Riffusion, GPT系）"]
    D -->|素材生成| B
    B -->|音響出力| E["観客（リアクションデータ）"]
    E -->|感情解析| C


</div>


---

## 結論：創作的新民主化

現場編碼是通過“通過和弦即興表達”。
他讓音樂成為任何人都可以創造的表演。
人工智能將進一步使其民主化，
我們正在努力創造一種**分享“表演智慧”的文化**。

算法與人類、機器與情感。
邊界融化的地方，
新的音樂地平線崛起。

> 和弦超越樂譜，人工智能學會即興創作。
> 音樂不再是“人類專有的專利”；
> **共同創意智慧**。

---
